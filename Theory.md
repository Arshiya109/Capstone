Introduction to Deep Learning
NAME:	Arshiya M. Saiyyad
ROLL NO.:	UGMR20230014
A. What is mean by Deep Learning?
Deep learning is a subfield of machine learning, which is a subset of artificial intelligence. It makes use of artificial neural networks, which are inspired by the structure and function of the human brain, to process and learn from massive volumes of data.
Important Features: 
Several Layers: Deep learning models, in contrast to conventional neural networks, comprise several layers (hence the name "deep") that enable them to learn and represent input at ever higher levels of abstraction.

Data-Driven: These models are appropriate for applications where data is plentiful because they need enormous volumes of data to learn efficiently.

Complex Feature Learning: By automatically identifying complex patterns in data, deep learning models do away with the requirement for human feature extraction.

Example:
You need to teach a computer to identify various animals in pictures. You would give it a huge library of images with labels (cats, dogs, horses, etc.). Through the use of several processing layers, the deep learning model would come to recognize the distinctive characteristics of every species.


B. Fundamental Ideas in Deep Learning
1. Layers and Neurons:
A neural network's fundamental building blocks, neurons receive inputs, process them, and forward the results to the subsequent layer. Neuronal groups called layers. Three primary categories exist:
Input Layer: Gets the first set of data.
Hidden levels: Data processing occurs in intermediate levels. There can be more than one buried layer.
Output Layer: Generates the last categorization or prediction.
2. Functions of Activation:
These processes decide whether a neuron needs to fire, or be stimulated. Tanh, Sigmoid, and ReLU (Rectified Linear Unit) are examples of common activation functions.

3. Education and Training:
Forward Propagation: Predictions are made by moving data from the input layer to the output layer across the network.
Backward Propagation: Using algorithms such as gradient descent to minimize error, the network modifies its weights in response to prediction mistakes.
4.Loss Function: 
A metric indicating how well the predictions made by the neural network match the real data. Reducing this loss is the aim of training.
C. CNNs, or Convolutional Neural Networks:
CNNs: What are they?
CNNs are a subset of deep learning models created especially to handle structured grid data, such as pictures. For tasks involving picture identification and classification, they are very successful.
Important Elements:
Convolutional Layers: These layers generate feature maps by applying filters to the input image. Certain features, including edges or textures, are detected by each filter.
Pooling Layers: These layers make computations more efficient while reducing the dimensionality of feature maps and preserving important information.
Fully Connected Layers: The output is flattened and sent through fully connected layers to get final predictions following a number of convolutional and pooling layers.
Example:
CNNs are capable of automatically learning to recognize different animal characteristics, such as fur patterns, shapes, and colors, and classifying photos based on those aspects.
D. RNNs, or recurrent neural networks:
RNNs: What are they?
Neural networks specifically developed to handle sequential data are called RNNs. They work especially well for tasks like speech recognition, natural language processing, and time series analysis where context and order are crucial.
Important Elements:
Recurrent connections: RNNs, in contrast to conventional neural networks, possess backtracking connections, which enable them to save information throughout different sequences. This facilitates digesting the present input by helping you recall the prior ones.
Hidden States: By storing data about earlier inputs, these states help the network comprehend context and order.
Example:
RNNs can be employed in language translation, where it's essential to comprehend the meaning of words that have come before in order to translate words accurately.

E.The Transformers:

Transformers: What Are They?

Transformers represent a novel and extremely sophisticated class of neural network architecture that works especially well for problems involving natural language processing. By enabling models to handle long-range dependencies and parallelize training, they have completely changed the field.

Important Elements:

Attention Mechanism:The attention mechanism enables the model to assess the relative importance of various segments of the input sequence, concentrating on the segments that are more pertinent. It facilitates the capture of dependencies and linkages in data.

Encoder-Decoder Structure: In language translation tasks, the input sequence is processed by the encoder, and the output sequence is generated by the decoder.

Example:

Transformers are employed in models such as GPT-3, which can comprehend and produce text that is human-like in order to carry out a variety of linguistic tasks, including as composing essays and responding to queries.

Github Collaboratory Link:

https://github.com/Arshiya109/Fundamentals-of-Deep-Learning
